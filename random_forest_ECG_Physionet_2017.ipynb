{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries<br>\n",
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import os  \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric \n",
    "from sklearn.metrics import confusion_matrix \n",
    "# Model \n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest model\n",
    "# preprocessing\n",
    "from imblearn.over_sampling import SMOTE  # Handling class imbalance\n",
    "import pywt  # wavelet transformations\n",
    "import scipy.io  # loading MATLAB files\n",
    "import neurokit2 as nk  #  ECG signal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/sane/Library/Mobile Documents/com~apple~CloudDocs/Master 2 IHU LIRYC/APP project/APP 1 - Atrial Fibrillation/Data/PhysionetAFDatabase/TrainingSet/REFERENCE.csv'\n",
    "file = pd.read_csv(path, delimiter=',', header=None)\n",
    "file.columns = ('filename', 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out unwanted classes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~file[\"target\"].str.contains('e')  # Exclude records with class 'e', # Classification : 'A|N|O|~'\n",
    "filtered_filenames = file[\"filename\"][mask]\n",
    "filtered_targets = file[\"target\"][mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/sane/Library/Mobile Documents/com~apple~CloudDocs/Master 2 IHU LIRYC/APP project/APP 1 - Atrial Fibrillation/Data/PhysionetAFDatabase/validationSet/REFERENCE.csv'\n",
    "file = pd.read_csv(path, delimiter=',', header=None)\n",
    "file.columns = ('filename', 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the same filtering to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_filenames_test = file[\"filename\"][mask]\n",
    "filtered_targets_test = file[\"target\"][mask]\n",
    "print(filtered_targets.shape, filtered_filenames.shape, filtered_filenames_test.shape, filtered_targets_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = filtered_filenames\n",
    "X_test = filtered_filenames_test\n",
    "y_train = filtered_targets\n",
    "y_test = filtered_targets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame = {'filename': X_train, 'target': y_train}\n",
    "pd_train = pd.DataFrame(train_frame)\n",
    "pd_train.groupby('target').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = {'filename': X_test, 'target': y_test}\n",
    "pd_val = pd.DataFrame(val)\n",
    "pd_val.groupby('target').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(X_train[:])\n",
    "data_train = []  # To store preprocessed training signals\n",
    "labels_train = []  # To store corresponding labels\n",
    "data_RR = []  # Placeholder for additional feature\n",
    "for i in range(len(a)):\n",
    "    AF = 0  # Default class\n",
    "\n",
    "    #  target labels to numerical values\n",
    "    if y_train.iloc[i] == 'N':\n",
    "        AF = 0\n",
    "    elif y_train.iloc[i] == 'A':\n",
    "        AF = 1\n",
    "    elif y_train.iloc[i] == 'O':\n",
    "        AF = 2\n",
    "    elif y_train.iloc[i] == '~':\n",
    "        AF = 3\n",
    "    else:\n",
    "        print('5 classes error')\n",
    "        break\n",
    "\n",
    "    # Load and preprocess ECG data\n",
    "    mat = scipy.io.loadmat(os.path.join('Data/PhysionetAFDatabase/TrainingSet', X_train.iloc[i]))\n",
    "    data = np.array(mat['val'][:][:]).flatten()\n",
    "    data = nk.ecg_clean(data, sampling_rate=300, method='pantompkins1985')  # Clean ECG data\n",
    "\n",
    "    # Standardize the data\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    data = (data - mean) / std\n",
    "\n",
    "    # Slice the data into overlapping windows\n",
    "    shape = data.shape\n",
    "    part = np.arange(0, shape[0] + 1, 3000)\n",
    "\n",
    "    # Skip signals with less than 3000 samples\n",
    "    if shape[0] <= 3000:\n",
    "        continue\n",
    "    for j in range(len(part) - 1):\n",
    "        sc_minus = part[j]\n",
    "        sc_plus = part[j + 1]\n",
    "        data_train.append(data[sc_minus:sc_plus])\n",
    "        labels_train.append(AF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert training data to NumPy arrays and handle class imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.asarray(data_train, 'float32')\n",
    "smote = SMOTE(sampling_strategy={3: 1500}, random_state=42, k_neighbors=10)  # Oversample class 3\n",
    "train, labels_train = smote.fit_resample(train, labels_train)\n",
    "print(train.shape)\n",
    "print(len(labels_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess test data similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(X_test[:])\n",
    "data_test = []  # To store preprocessed test signals\n",
    "labels_test = []  # To store corresponding test labels\n",
    "for i in range(len(a)):\n",
    "    if y_test.iloc[i] == 'N':\n",
    "        AF = 0\n",
    "    elif y_test.iloc[i] == 'A':\n",
    "        AF = 1\n",
    "    elif y_test.iloc[i] == 'O':\n",
    "        AF = 2\n",
    "    elif y_test.iloc[i] == '~':\n",
    "        AF = 3\n",
    "    else:\n",
    "        print('5 classes error')\n",
    "        break\n",
    "\n",
    "    # Load and preprocess ECG data\n",
    "    mat = scipy.io.loadmat(os.path.join('Data/PhysionetAFDatabase/validationSet', X_test.iloc[i]))\n",
    "    data = np.array(mat['val'][:][:]).flatten()\n",
    "    data = nk.ecg_clean(data, sampling_rate=300, method='pantompkins1985')\n",
    "\n",
    "    # Standardize the data\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    data = (data - mean) / std\n",
    "\n",
    "    # Slice test data into windows of size 3000\n",
    "    if shape[0] >= 6000:\n",
    "        labels_test.append(AF)\n",
    "        data_test.append(data[3000:6000])\n",
    "    elif shape[0] >= 3000:\n",
    "        labels_test.append(AF)\n",
    "        data_test.append(data[0:3000])\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of data test is: {np.array(data_test).shape}')\n",
    "test = np.asarray(data_test, 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for feature extraction using wavelet decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    features = []\n",
    "    for sig in data:\n",
    "        coeffs = pywt.wavedec(sig, 'db4', level=4)  # Wavelet decomposition\n",
    "        mean_features = [np.expand_dims(np.mean(c), axis=0) for c in coeffs]  # Compute mean of coefficients\n",
    "        features.append(np.concatenate(mean_features))\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(train)  # Extract features for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=32, verbose=2, n_jobs=-1)\n",
    "model.fit(features, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict(extract_features(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_confusion_matrix = confusion_matrix(labels_test, y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate F1 scores for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_N_predicted = np.sum(combined_confusion_matrix[:, 0])\n",
    "sum_A_predicted = np.sum(combined_confusion_matrix[:, 1])\n",
    "sum_other_predicted = np.sum(combined_confusion_matrix[:, 2])\n",
    "sum_noise_predicted = np.sum(combined_confusion_matrix[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_N_true = np.sum(combined_confusion_matrix[0, :])\n",
    "sum_A_true = np.sum(combined_confusion_matrix[1, :])\n",
    "sum_other_true = np.sum(combined_confusion_matrix[2, :])\n",
    "sum_noise_true = np.sum(combined_confusion_matrix[3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_N = 2 * combined_confusion_matrix[0, 0] / (sum_N_predicted + sum_N_true)\n",
    "f1_A = 2 * combined_confusion_matrix[1, 1] / (sum_A_predicted + sum_A_true)\n",
    "f1_other = 2 * combined_confusion_matrix[2, 2] / (sum_other_predicted + sum_other_true)\n",
    "f1_noise = 2 * combined_confusion_matrix[3, 3] / (sum_noise_predicted + sum_noise_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = (f1_noise + f1_A + f1_N + f1_other) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print F1 scores and plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_N, f1_N, f1_other, f1_noise, f1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(combined_confusion_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'AF', 'Other', 'Noisy'], \n",
    "            yticklabels=['Normal', 'AF', 'Other', 'Noisy'])\n",
    "plt.title(\"Confusion Matrix for All Classes\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
